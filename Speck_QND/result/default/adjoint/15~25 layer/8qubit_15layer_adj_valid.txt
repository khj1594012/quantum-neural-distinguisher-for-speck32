hj@DESKTOP-6RMLMKU:~/SPECK$ python3 valid_8qubit_15layer_adj.py
2023-03-03 12:19:40.982666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-03-03 12:19:40.983562: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-03-03 12:20:00.390808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-03-03 15:06:57.533669: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2023-03-03 15:06:57.534508: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-6RMLMKU): /proc/driver/nvidia/version does not exist
2023-03-03 15:06:57.535954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 64)]         0           []

 tf.split (TFOpLambda)          [(None, 8),          0           ['input_1[0][0]']
                                 (None, 8),
                                 (None, 8),
                                 (None, 8),
                                 (None, 8),
                                 (None, 8),
                                 (None, 8),
                                 (None, 8)]

 keras_layer_1 (KerasLayer)     (None, 8)            120         ['tf.split[0][0]']

 keras_layer_2 (KerasLayer)     (None, 8)            120         ['tf.split[0][1]']

 keras_layer_3 (KerasLayer)     (None, 8)            120         ['tf.split[0][2]']

 keras_layer_4 (KerasLayer)     (None, 8)            120         ['tf.split[0][3]']

 keras_layer_5 (KerasLayer)     (None, 8)            120         ['tf.split[0][4]']

 keras_layer_6 (KerasLayer)     (None, 8)            120         ['tf.split[0][5]']

 keras_layer_7 (KerasLayer)     (None, 8)            120         ['tf.split[0][6]']

 keras_layer_8 (KerasLayer)     (None, 8)            120         ['tf.split[0][7]']

 tf.concat (TFOpLambda)         (None, 64)           0           ['keras_layer_1[0][0]',
                                                                  'keras_layer_2[0][0]',
                                                                  'keras_layer_3[0][0]',
                                                                  'keras_layer_4[0][0]',
                                                                  'keras_layer_5[0][0]',
                                                                  'keras_layer_6[0][0]',
                                                                  'keras_layer_7[0][0]',
                                                                  'keras_layer_8[0][0]']

 dense_1 (Dense)                (None, 64)           4160        ['tf.concat[0][0]']

 batch_normalization (BatchNorm  (None, 64)          256         ['dense_1[0][0]']
 alization)

 activation (Activation)        (None, 64)           0           ['batch_normalization[0][0]']

 dense_3 (Dense)                (None, 1)            65          ['activation[0][0]']

==================================================================================================
Total params: 5,441
Trainable params: 5,313
Non-trainable params: 128
__________________________________________________________________________________________________
Epoch 1/10



313/313 - 9408s - loss: 0.7216 - acc: 0.4952 - val_loss: 0.7063 - val_acc: 0.5024 - 9408s/epoch - 30s/step
Epoch 2/10
313/313 - 9347s - loss: 0.6956 - acc: 0.5293 - val_loss: 0.7065 - val_acc: 0.5028 - 9347s/epoch - 30s/step
Epoch 3/10
313/313 - 9326s - loss: 0.6872 - acc: 0.5451 - val_loss: 0.7066 - val_acc: 0.5062 - 9326s/epoch - 30s/step
Epoch 4/10
313/313 - 9368s - loss: 0.6786 - acc: 0.5693 - val_loss: 0.7074 - val_acc: 0.4962 - 9368s/epoch - 30s/step
Epoch 5/10
313/313 - 9384s - loss: 0.6732 - acc: 0.5853 - val_loss: 0.7080 - val_acc: 0.5098 - 9384s/epoch - 30s/step
Epoch 6/10
313/313 - 9394s - loss: 0.6658 - acc: 0.5985 - val_loss: 0.7043 - val_acc: 0.5200 - 9394s/epoch - 30s/step
Epoch 7/10
313/313 - 9328s - loss: 0.6290 - acc: 0.6613 - val_loss: 0.6208 - val_acc: 0.6632 - 9328s/epoch - 30s/step
Epoch 8/10
313/313 - 9251s - loss: 0.5874 - acc: 0.6960 - val_loss: 0.6195 - val_acc: 0.6758 - 9251s/epoch - 30s/step
Epoch 9/10
313/313 - 9259s - loss: 0.5766 - acc: 0.7042 - val_loss: 0.6219 - val_acc: 0.6662 - 9259s/epoch - 30s/step
Epoch 10/10
313/313 - 8850s - loss: 0.5691 - acc: 0.7109 - val_loss: 0.6290 - val_acc: 0.6556 - 8850s/epoch - 28s/step
32/32 [==============================] - 349s 11s/step
0.656