hj@DESKTOP-6RMLMKU~SPECK$ python3 16qubit_5layers_nodense.py
2023-03-08 094017.925048 W tensorflowstream_executorplatformdefaultdso_loader.cc64] Could not load dynamic library 'libcudart.so.11.0'; dlerror libcudart.so.11.0 cannot open shared object file No such file or directory
2023-03-08 094017.925530 I tensorflowstream_executorcudacudart_stub.cc29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-03-08 094036.021805 W tensorflowstream_executorplatformdefaultdso_loader.cc64] Could not load dynamic library 'libcuda.so.1'; dlerror libcuda.so.1 cannot open shared object file No such file or directory
2023-03-08 094036.022289 W tensorflowstream_executorcudacuda_driver.cc269] failed call to cuInit UNKNOWN ERROR (303)
2023-03-08 094036.022551 I tensorflowstream_executorcudacuda_diagnostics.cc156] kernel driver does not appear to be running on this host (DESKTOP-6RMLMKU) procdrivernvidiaversion does not exist
2023-03-08 094036.023629 I tensorflowcoreplatformcpu_feature_guard.cc193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model model
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 64)]         0           []

 tf.split (TFOpLambda)          [(None, 16),         0           ['input_1[0][0]']
                                 (None, 16),
                                 (None, 16),
                                 (None, 16)]

 keras_layer_1 (KerasLayer)     (None, 16)           80          ['tf.split[0][0]']

 keras_layer_2 (KerasLayer)     (None, 16)           80          ['tf.split[0][1]']

 keras_layer_3 (KerasLayer)     (None, 16)           80          ['tf.split[0][2]']

 keras_layer_4 (KerasLayer)     (None, 16)           80          ['tf.split[0][3]']

 tf.concat (TFOpLambda)         (None, 64)           0           ['keras_layer_1[0][0]',
                                                                  'keras_layer_2[0][0]',
                                                                  'keras_layer_3[0][0]',
                                                                  'keras_layer_4[0][0]']

 dense_1 (Dense)                (None, 1)            65          ['tf.concat[0][0]']

==================================================================================================
Total params 385
Trainable params 385
Non-trainable params 0
__________________________________________________________________________________________________
Epoch 110
313313 - 7394s - loss 0.7664 - acc 0.4981 - val_loss 0.7229 - val_acc 0.5058 - 7394sepoch - 24sstep
Epoch 210
313313 - 7373s - loss 0.7110 - acc 0.5027 - val_loss 0.7019 - val_acc 0.5040 - 7373sepoch - 24sstep
Epoch 310
313313 - 7417s - loss 0.6980 - acc 0.5063 - val_loss 0.6963 - val_acc 0.5028 - 7417sepoch - 24sstep
Epoch 410
313313 - 7404s - loss 0.6942 - acc 0.5106 - val_loss 0.6948 - val_acc 0.5060 - 7404sepoch - 24sstep
Epoch 510
313313 - 7379s - loss 0.6928 - acc 0.5090 - val_loss 0.6944 - val_acc 0.5004 - 7379sepoch - 24sstep
Epoch 610
313313 - 7382s - loss 0.6922 - acc 0.5223 - val_loss 0.6942 - val_acc 0.5044 - 7382sepoch - 24sstep
Epoch 710
313313 - 7397s - loss 0.6918 - acc 0.5188 - val_loss 0.6942 - val_acc 0.5068 - 7397sepoch - 24sstep
Epoch 810
313313 - 7402s - loss 0.6916 - acc 0.5190 - val_loss 0.6942 - val_acc 0.5054 - 7402sepoch - 24sstep
Epoch 910
313313 - 7392s - loss 0.6915 - acc 0.5226 - val_loss 0.6944 - val_acc 0.5056 - 7392sepoch - 24sstep
Epoch 1010
313313 - 7377s - loss 0.6914 - acc 0.5209 - val_loss 0.6945 - val_acc 0.5040 - 7377sepoch - 24sstep
3232 [==============================] - 152s 5sstep
0.521